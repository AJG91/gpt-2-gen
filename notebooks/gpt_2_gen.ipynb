{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f6d6fe-b7f0-4e52-88bf-6aaf84d4674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch as tc\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "AutoModelForCausalLM, AutoTokenizer, \n",
    "DataCollatorForLanguageModeling, Trainer, \n",
    "TrainingArguments\n",
    ")\n",
    "\n",
    "from gpt_2_gen.utils import to_device, get_device, set_seed, generate_namespace\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97740a42-b9fd-43de-907b-9e503ec9f558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model_name\": \"gpt2\",\n",
      "  \"prompt\": \"The future of physics is\",\n",
      "  \"seed\": 42,\n",
      "  \"max_token_len\": 128,\n",
      "  \"train_size\": 2000,\n",
      "  \"val_size\": 1000,\n",
      "  \"lr\": 2e-05,\n",
      "  \"epochs\": 3,\n",
      "  \"train_batch_size\": 4,\n",
      "  \"eval_batch_size\": 4,\n",
      "  \"save_step\": 250,\n",
      "  \"eval_step\": 250,\n",
      "  \"log_step\": 100,\n",
      "  \"strategy\": \"epoch\",\n",
      "  \"fig_path\": \"../outputs/\",\n",
      "  \"res_path\": \"../results/\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cfg = generate_namespace(path=f\"../config.yaml\")\n",
    "print(json.dumps(vars(cfg), indent=2))\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7867a36-5d22-4d8a-bd28-33590dfe1fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  464,  2003,   286, 11887,   318]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1]], device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(cfg.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"left\"\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.to(device)\n",
    "\n",
    "inputs = tokenizer(\n",
    "    cfg.prompt,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "inputs = to_device(inputs, device)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8f33a52-aa4e-47b3-9c56-c787080e80f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of physics is in the hands of the people who are going to make it happen.\n",
      "\n",
      "\"We're going to have to make sure that we're not just going to be able to do this, but we're going to have to make sure that we're\n"
     ]
    }
   ],
   "source": [
    "with tc.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=50,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "for seq in outputs: \n",
    "    print(tokenizer.decode(seq, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2983c6-34da-404f-a886-4f5ceb3b2e99",
   "metadata": {},
   "source": [
    "## Parameters explanation:\n",
    "    1. **inputs\n",
    "        * Actually splits up into\n",
    "            - input_ids=inputs[\"input_ids\"],\n",
    "            - attention_mask=inputs[\"attention_mask\"]\n",
    "    2. do_sample\n",
    "        * Tells model to sample from probability distribution instead of picking the highest-probability token.\n",
    "        do_sample = False -> deterministic\n",
    "        do_sample = True -> random sampling\n",
    "    3. top_k\n",
    "        * Sorts logits and keepstop 50 most probably tokens. Then samples from here.\n",
    "        * Lower k -> more deterministic\n",
    "        * Higher k -> more random\n",
    "    4. top_p = 0.95\n",
    "        * Keeps the smallest set of tokens whose cumulative probability >= 0.95\n",
    "        * Can be used in conjunction with top_k, which amounts to picking tokens that intersect with these two conditions.\n",
    "        * Helps adapt to uncertainty.\n",
    "    5. temperature\n",
    "        * Scales softmax distribution before sampling\n",
    "        * Lower T -> more deterministic\n",
    "        * Higher T -> more random\n",
    "    6. num_return_sequences\n",
    "        * Generates N number of separate continuations (predictions) for the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "616f8a33-5f71-4674-bd43-4d37a0267109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq. 1:\n",
      "The future of physics is still in its infancy, but with a lot of new knowledge, new technologies, and new ideas, it is likely that the next big breakthrough will be the development of a particle accelerator.\n",
      "\n",
      "In this post, we will explore the idea of a\n",
      "\n",
      "\n",
      "Seq. 2:\n",
      "The future of physics is not the same as it was twenty-five years ago. It is not an easy time to live in, and it is not an easy place to grow up. It is not the way of life we know today.\n",
      "\n",
      "Today I am writing\n",
      "\n",
      "\n",
      "Seq. 3:\n",
      "The future of physics is going to be interesting, but it has to be a lot more interesting than what we've been doing,\" said one of the researchers involved in the work, J.D. Smith. \"We need to understand and improve on things that are not being\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tc.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=3,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "for i, seq in enumerate(outputs):\n",
    "    print(f\"Seq. {i + 1}:\")\n",
    "    print(f\"{tokenizer.decode(seq, skip_special_tokens=True)}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a42565e-10e6-42e4-85c3-913e946eeda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d41e2dc05a441bba113d9a4af0d694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4948e8719f754dceb8ab43a92e0773c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887741f5234849ef9f33308b91ff8a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = \"imdb\"\n",
    "dataset = load_dataset(data)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    tokenized = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=cfg.max_token_len,\n",
    "    )\n",
    "    return tokenized\n",
    "\n",
    "data_tokenized = dataset.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5c7065f-78a1-4cd7-ac49-e78a69fd9c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 20:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.688700</td>\n",
       "      <td>3.654404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.577800</td>\n",
       "      <td>3.653589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.516500</td>\n",
       "      <td>3.658192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=3.6088597819010415, metrics={'train_runtime': 1213.7658, 'train_samples_per_second': 4.943, 'train_steps_per_second': 1.236, 'total_flos': 391938048000000.0, 'train_loss': 3.6088597819010415, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=cfg.res_path,\n",
    "    eval_strategy=cfg.strategy,\n",
    "    save_strategy= cfg.strategy,\n",
    "    learning_rate=cfg.lr,\n",
    "    save_steps=cfg.save_step,\n",
    "    eval_steps=cfg.eval_step,\n",
    "    logging_steps=cfg.log_step,\n",
    "    per_device_train_batch_size=cfg.train_batch_size,\n",
    "    per_device_eval_batch_size=cfg.eval_batch_size,\n",
    "    num_train_epochs=cfg.epochs,\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"none\",\n",
    "    seed=cfg.seed,\n",
    "    use_cpu=False if device.type == \"mps\" else True\n",
    ")\n",
    "\n",
    "# High training wrapper for Hugging Face\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data_tokenized[\"train\"].shuffle(seed=cfg.seed).select(range(cfg.train_size)),\n",
    "    eval_dataset=data_tokenized[\"test\"].shuffle(seed=cfg.seed).select(range(cfg.val_size)),\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "if device.type == \"mps\":\n",
    "    tc.mps.empty_cache()\n",
    "    \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e11af36c-1545-4c89-8476-efae470a27ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 38.79\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "eval_results = trainer.evaluate()\n",
    "eval_loss = eval_results[\"eval_loss\"]\n",
    "perplexity = exp(eval_loss)\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e66a3c-3378-4f57-94a5-782e5641c32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71d63794-de41-4169-98b3-756fe57b1f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trainer.state.best_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ba0b891-2683-42ae-b860-80b2a49789cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_ckpt = trainer.state.best_model_checkpoint\n",
    "# print(\"Best checkpoint:\", best_ckpt)\n",
    "\n",
    "# # Check existence and file size\n",
    "# if best_ckpt and os.path.exists(os.path.join(best_ckpt, \"pytorch_model.bin\")):\n",
    "#     print(\"Checkpoint found and looks valid.\")\n",
    "# else:\n",
    "#     print(\"Checkpoint missing or incomplete â€” falling back to final model.\")\n",
    "#     best_ckpt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a6eae2d-b66e-4fb6-b2fe-2808e35d7eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM\n",
    "\n",
    "# if best_ckpt:\n",
    "#     best_model = AutoModelForCausalLM.from_pretrained(best_ckpt)\n",
    "# else:\n",
    "#     best_model = trainer.model  # fallback to last trained model\n",
    "\n",
    "# best_model.to(device)\n",
    "# best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "629d5efd-a213-48bc-a390-449cde3ba747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import logging\n",
    "# logging.set_verbosity_info()\n",
    "\n",
    "# best_model = AutoModelForCausalLM.from_pretrained(best_ckpt, ignore_mismatched_sizes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18a715f1-2f52-4a11-bdf5-9ea962f54ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model.save_pretrained(cfg.res_path + \"best_model_clean\")\n",
    "# tokenizer.save_pretrained(cfg.res_path + \"best_model_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b287d7-a5f9-4e0f-ab2c-9596bdbf3b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
