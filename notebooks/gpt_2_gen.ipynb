{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f6d6fe-b7f0-4e52-88bf-6aaf84d4674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch as tc\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorWithPadding\n",
    "from gpt_2_gen.utils import to_device, get_device, set_seed, generate_namespace\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97740a42-b9fd-43de-907b-9e503ec9f558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model_name\": \"gpt2\",\n",
      "  \"prompt\": \"The future of physics is\",\n",
      "  \"seed\": 42,\n",
      "  \"dpi\": 400,\n",
      "  \"fig_path\": \"../outputs/\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cfg = generate_namespace(path=f\"../config.yaml\")\n",
    "print(json.dumps(vars(cfg), indent=2))\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1de08a1-32d6-4070-9953-25686457e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.model_name\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7867a36-5d22-4d8a-bd28-33590dfe1fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  464,  2003,   286, 11887,   318]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1]], device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "inputs = tokenizer(\n",
    "    cfg.prompt,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "inputs = to_device(inputs, device)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f33a52-aa4e-47b3-9c56-c787080e80f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of physics is in the hands of the people who are going to make it happen.\n",
      "\n",
      "\"We're going to have to make sure that we're not just going to be able to do this, but we're going to have to make sure that we're\n"
     ]
    }
   ],
   "source": [
    "with tc.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=50,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "for seq in outputs: \n",
    "    print(tokenizer.decode(seq, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2983c6-34da-404f-a886-4f5ceb3b2e99",
   "metadata": {},
   "source": [
    "## Parameters explanation:\n",
    "    1. **inputs\n",
    "        * Actually splits up into\n",
    "            - input_ids=inputs[\"input_ids\"],\n",
    "            - attention_mask=inputs[\"attention_mask\"]\n",
    "    2. do_sample\n",
    "        * Tells model to sample from probability distribution instead of picking the highest-probability token.\n",
    "        do_sample = False -> deterministic\n",
    "        do_sample = True -> random sampling\n",
    "    3. top_k\n",
    "        * Sorts logits and keepstop 50 most probably tokens. Then samples from here.\n",
    "        * Lower k -> more deterministic\n",
    "        * Higher k -> more random\n",
    "    4. top_p = 0.95\n",
    "        * Keeps the smallest set of tokens whose cumulative probability >= 0.95\n",
    "        * Can be used in conjunction with top_k, which amounts to picking tokens that intersect with these two conditions.\n",
    "        * Helps adapt to uncertainty.\n",
    "    5. temperature\n",
    "        * Scales softmax distribution before sampling\n",
    "        * Lower T -> more deterministic\n",
    "        * Higher T -> more random\n",
    "    6. num_return_sequences\n",
    "        * Generates N number of separate continuations (predictions) for the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "616f8a33-5f71-4674-bd43-4d37a0267109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq. 1:\n",
      "The future of physics is still in its infancy, but with a lot of new knowledge, new technologies, and new ideas, it is likely that the next big breakthrough will be the development of a particle accelerator.\n",
      "\n",
      "In this post, we will explore the idea of a\n",
      "\n",
      "\n",
      "Seq. 2:\n",
      "The future of physics is not the same as it was twenty-five years ago. It is not an easy time to live in, and it is not an easy place to grow up. It is not the way of life we know today.\n",
      "\n",
      "Today I am writing\n",
      "\n",
      "\n",
      "Seq. 3:\n",
      "The future of physics is going to be interesting, but it has to be a lot more interesting than what we've been doing,\" said one of the researchers involved in the work, J.D. Smith. \"We need to understand and improve on things that are not being\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tc.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=3,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "for i, seq in enumerate(outputs):\n",
    "    print(f\"Seq. {i + 1}:\")\n",
    "    print(f\"{tokenizer.decode(seq, skip_special_tokens=True)}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf93ff82-9d11-4a92-abae-c385e9a79078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b2bec99b794b47a81cccba4b96352a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "data_tokenized = dataset.map(\n",
    "    lambda x: tokenizer(x[\"text\"], truncation=True, max_length=128), \n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     tokenized[\"train\"],\n",
    "#     batch_size=16,\n",
    "#     shuffle=True,\n",
    "#     collate_fn=data_collator\n",
    "# )\n",
    "\n",
    "# eval_loader = DataLoader(\n",
    "#     tokenized[\"test\"],\n",
    "#     batch_size=16,\n",
    "#     collate_fn=data_collator\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c7065f-78a1-4cd7-ac49-e78a69fd9c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
