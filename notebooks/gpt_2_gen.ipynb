{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f6d6fe-b7f0-4e52-88bf-6aaf84d4674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\\\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d696eef5-02be-4631-bbef-93a04fd7d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(inputs, device):\n",
    "    return {k: (v.to(device) if tc.is_tensor(v) else v) for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1de08a1-32d6-4070-9953-25686457e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = tc.device(\"mps\") if tc.backends.mps.is_available() else tc.device(\"cpu\")\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "prompt = \"The future of physics is\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7867a36-5d22-4d8a-bd28-33590dfe1fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  464,  2003,   286, 11887,   318]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1]], device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "inputs = to_device(inputs, device)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f33a52-aa4e-47b3-9c56-c787080e80f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of physics is in the hands of the people who are going to make it happen.\n",
      "\n",
      "\"We're going to have to make sure that we're not just going to be able to do this, but we're going to have to make sure that we're\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "with tc.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=50,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "for seq in outputs: \n",
    "    print(tokenizer.decode(seq, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616f8a33-5f71-4674-bd43-4d37a0267109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
